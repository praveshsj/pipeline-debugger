"""
Database models for the Pipeline Debugger.

This module defines the core data models:
- Incident: A pipeline failure event
- Evidence: Collected context (logs, commits, schema changes)
- Fix: Generated solutions for incidents
- Pattern: Learned patterns for similar incidents (with vector embeddings)
"""

from datetime import datetime
from typing import Optional, List
from sqlalchemy import Column, Integer, String, DateTime, Text, Boolean, JSON, ForeignKey, Float
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID, ARRAY
from pgvector.sqlalchemy import Vector
import uuid

from app.core.database import Base


class Incident(Base):
    """
    Represents a single pipeline failure incident.
    Core entity that ties together all evidence, analysis, and fixes.
    """
    __tablename__ = "incidents"
    
    # Primary key
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Pipeline identification
    dag_id = Column(String(255), nullable=False, index=True)
    task_id = Column(String(255), nullable=False, index=True)
    execution_date = Column(DateTime, nullable=False, index=True)
    
    # Orchestrator info
    orchestrator = Column(String(50), default="airflow")  # airflow, prefect, dagster
    orchestrator_url = Column(String(500))  # Link back to orchestrator UI
    
    # Error information
    error_message = Column(Text)
    error_type = Column(String(255))  # Exception class name
    stack_trace = Column(Text)
    
    # Status tracking
    status = Column(
        String(50), 
        default="pending",
        index=True
    )  # pending, analyzing, analyzed, fixing, fixed, failed
    
    # Root cause analysis (populated by Claude)
    root_cause = Column(Text)
    root_cause_confidence = Column(Float)  # 0.0 to 1.0
    analysis_summary = Column(Text)
    
    # Impact assessment
    affected_downstream_tasks = Column(ARRAY(String))  # List of task IDs
    data_gap_hours = Column(Float)  # Hours of missing data
    estimated_backfill_cost_usd = Column(Float)
    
    # Timing
    detected_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    analyzed_at = Column(DateTime)
    resolved_at = Column(DateTime)
    
    # Relationships
    evidence = relationship("Evidence", back_populates="incident", cascade="all, delete-orphan")
    fixes = relationship("Fix", back_populates="incident", cascade="all, delete-orphan")
    
    # Metadata
    metadata_json = Column(JSON, default={})  # Flexible storage for extra data
    
    def __repr__(self):
        return f"<Incident {self.dag_id}.{self.task_id} at {self.execution_date}>"


class Evidence(Base):
    """
    Evidence collected for an incident.
    Stores logs, git commits, schema changes, and other context.
    """
    __tablename__ = "evidence"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    incident_id = Column(UUID(as_uuid=True), ForeignKey("incidents.id"), nullable=False, index=True)
    
    # Evidence classification
    evidence_type = Column(
        String(50), 
        nullable=False,
        index=True
    )  # log, git_commit, schema_change, upstream_status, similar_incident
    
    # Content
    content = Column(Text)  # The actual evidence (log text, commit diff, etc)
    source = Column(String(255))  # Where it came from
    
    # Relevance scoring (for filtering)
    relevance_score = Column(Float)  # 0.0 to 1.0, how relevant this evidence is
    
    # Timing
    collected_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    evidence_timestamp = Column(DateTime)  # When the evidence was created
    
    # Metadata
    metadata_json = Column(JSON, default={})
    
    # Relationships
    incident = relationship("Incident", back_populates="evidence")
    
    def __repr__(self):
        return f"<Evidence {self.evidence_type} for incident {self.incident_id}>"


class Fix(Base):
    """
    A proposed or applied fix for an incident.
    Generated by Claude AI, may include code patches, SQL, config changes.
    """
    __tablename__ = "fixes"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    incident_id = Column(UUID(as_uuid=True), ForeignKey("incidents.id"), nullable=False, index=True)
    
    # Fix details
    fix_type = Column(String(50))  # sql_patch, python_patch, config_change, manual_instruction
    description = Column(Text)  # Human-readable explanation
    code_patch = Column(Text)  # The actual fix code
    
    # Application tracking
    status = Column(
        String(50),
        default="proposed",
        index=True
    )  # proposed, applied, testing, verified, failed
    
    # GitHub integration
    github_pr_url = Column(String(500))
    github_pr_number = Column(Integer)
    github_branch_name = Column(String(255))
    
    # Success tracking
    applied_at = Column(DateTime)
    verified_at = Column(DateTime)
    success = Column(Boolean)  # Did this fix actually work?
    
    # If fix failed, why?
    failure_reason = Column(Text)
    
    # Confidence and priority
    confidence_score = Column(Float)  # 0.0 to 1.0, how confident Claude is
    priority = Column(Integer, default=1)  # If multiple fixes, which to try first
    
    # Timing
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    # Metadata
    metadata_json = Column(JSON, default={})
    
    # Relationships
    incident = relationship("Incident", back_populates="fixes")
    
    def __repr__(self):
        return f"<Fix {self.fix_type} for incident {self.incident_id}>"


class Pattern(Base):
    """
    Learned patterns from past incidents for faster diagnosis.
    Uses pgvector for similarity search of error patterns.
    """
    __tablename__ = "patterns"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Pattern identification
    name = Column(String(255))
    description = Column(Text)
    
    # Pattern signature (for matching)
    error_pattern = Column(Text)  # Regex or text pattern
    error_embedding = Column(Vector(1536))  # OpenAI embedding for similarity search
    
    # What causes this pattern
    common_root_causes = Column(ARRAY(String))
    
    # How to fix it
    fix_templates = Column(JSON)  # List of fix strategies
    
    # Learning metadata
    occurrence_count = Column(Integer, default=1)  # How many times we've seen this
    success_rate = Column(Float)  # How often our fixes work
    avg_resolution_time_minutes = Column(Float)
    
    # Related incidents (for reference)
    related_incident_ids = Column(ARRAY(UUID(as_uuid=True)))
    
    # Timing
    first_seen_at = Column(DateTime, default=datetime.utcnow)
    last_seen_at = Column(DateTime, default=datetime.utcnow)
    
    # Metadata
    metadata_json = Column(JSON, default={})
    
    def __repr__(self):
        return f"<Pattern {self.name} (seen {self.occurrence_count}x)>"


class User(Base):
    """
    User accounts (for multi-tenant support in future).
    MVP might skip auth and just have a single implicit user.
    """
    __tablename__ = "users"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String(255), unique=True, nullable=False, index=True)
    hashed_password = Column(String(255))
    
    # User settings
    github_username = Column(String(255))
    airflow_url = Column(String(500))
    
    # Status
    is_active = Column(Boolean, default=True)
    is_superuser = Column(Boolean, default=False)
    
    # Timing
    created_at = Column(DateTime, default=datetime.utcnow)
    last_login_at = Column(DateTime)
    
    def __repr__(self):
        return f"<User {self.email}>"
